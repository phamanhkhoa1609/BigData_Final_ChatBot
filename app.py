import streamlit as st
import os
import logging
import sys
from dotenv import load_dotenv
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    SummaryIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="H·ªá th·ªëng H·ªèi ƒê√°p T√†i Li·ªáu s·ª≠ d·ª•ng Gemini & LlamaIndex",
    page_icon="üìö",
    layout="wide",
)

# Custom CSS
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .response-container {
        background-color: #1E1E1E;
        border-radius: 5px;
        padding: 20px;
        margin: 10px 0;
    }
    .source-container {
        background-color: #2D2D2D;
        border-radius: 5px;
        padding: 10px;
        margin-top: 10px;
    }
    code {
        color: #4CAF50 !important;
    }
</style>
""", unsafe_allow_html=True)

# Ti√™u ƒë·ªÅ
st.title("H·ªá th·ªëng H·ªèi ƒê√°p T√†i Li·ªáu s·ª≠ d·ª•ng Gemini ")
st.caption("ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung c√°c file ")

# Kh·ªüi t·∫°o session state
if 'query_engine' not in st.session_state:
    # Load environment variables
    load_dotenv()
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        st.error("API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t. H√£y t·∫°o file .env v√† th√™m GEMINI_API_KEY='YOUR_API_KEY'")
        st.stop()

    # C·∫•u h√¨nh logging
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)

    with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
        # Kh·ªüi t·∫°o m√¥ h√¨nh
        Settings.llm = Gemini(api_key=GEMINI_API_KEY, model_name="models/gemini-2.0-flash")
        Settings.embed_model = GeminiEmbedding(
            api_key=GEMINI_API_KEY, model_name="models/embedding-001"
        )

        # Load v√† x·ª≠ l√Ω t√†i li·ªáu
        DATA_DIR = "data"
        try:
            reader = SimpleDirectoryReader(input_dir=DATA_DIR, recursive=True)
            documents = reader.load_data()
            
            # Ph√¢n t√°ch t√†i li·ªáu
            node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
            nodes = node_parser.get_nodes_from_documents(documents)

            # T·∫°o ho·∫∑c t·∫£i index
            PERSIST_DIR_VECTOR = "./storage_vector"
            PERSIST_DIR_SUMMARY = "./storage_summary"

            # Vector Index
            if not os.path.exists(PERSIST_DIR_VECTOR):
                vector_index = VectorStoreIndex(nodes, show_progress=True)
                vector_index.storage_context.persist(persist_dir=PERSIST_DIR_VECTOR)
            else:
                storage_context_vector = StorageContext.from_defaults(persist_dir=PERSIST_DIR_VECTOR)
                vector_index = load_index_from_storage(storage_context_vector)

            # Summary Index
            if not os.path.exists(PERSIST_DIR_SUMMARY):
                summary_index = SummaryIndex(nodes, show_progress=True)
                summary_index.storage_context.persist(persist_dir=PERSIST_DIR_SUMMARY)
            else:
                storage_context_summary = StorageContext.from_defaults(persist_dir=PERSIST_DIR_SUMMARY)
                summary_index = load_index_from_storage(storage_context_summary)

            # T·∫°o query engines
            vector_query_engine = vector_index.as_query_engine(similarity_top_k=3)
            summary_query_engine = summary_index.as_query_engine(
                response_mode="tree_summarize", use_async=True
            )

            # T·∫°o tools
            vector_tool = QueryEngineTool.from_defaults(
                query_engine=vector_query_engine,
                name="vector_search_tool",
                description="H·ªØu √≠ch ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin c·ª• th·ªÉ t·ª´ t√†i li·ªáu",
            )
            summary_tool = QueryEngineTool.from_defaults(
                query_engine=summary_query_engine,
                name="summary_tool",
                description="H·ªØu √≠ch ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung c·ªßa t√†i li·ªáu",
            )

            # T·∫°o router query engine
            st.session_state.query_engine = RouterQueryEngine(
                selector=LLMSingleSelector.from_defaults(llm=Settings.llm),
                query_engine_tools=[vector_tool, summary_tool],
                verbose=True,
            )

        except Exception as e:
            st.error(f"L·ªói khi kh·ªüi t·∫°o h·ªá th·ªëng: {str(e)}")
            st.stop()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ c√°c t√†i li·ªáu?"}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# X·ª≠ l√Ω input
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        try:
            with st.spinner("ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
                response = st.session_state.query_engine.query(prompt)
                
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi trong container v·ªõi style
                st.markdown('<div class="response-container">', unsafe_allow_html=True)
                st.markdown("**Question:** " + prompt)
                st.markdown("**Answer:**")
                st.code(response.response, language="")
                
                # Hi·ªÉn th·ªã metadata v√† source nodes
                if hasattr(response, "metadata"):
                    st.markdown("**Attributes:**")
                    st.code(f"response: The response text.\nmetadata: {response.metadata}", language="python")
                
                if hasattr(response, "source_nodes") and response.source_nodes:
                    st.markdown("**Source Nodes:**")
                    for node in response.source_nodes:
                        with st.container():
                            st.markdown('<div class="source-container">', unsafe_allow_html=True)
                            st.code(
                                f"file_name: {node.metadata.get('file_name', 'N/A')}\n"
                                f"file_path: {node.metadata.get('file_path', 'N/A')}\n"
                                f"score: {node.score:.4f}",
                                language="python"
                            )
                            st.markdown("</div>", unsafe_allow_html=True)
                
                st.markdown("</div>", unsafe_allow_html=True)
            
            # Th√™m c√¢u tr·∫£ l·ªùi v√†o l·ªãch s·ª≠
            st.session_state.messages.append({"role": "assistant", "content": response.response})
        except Exception as e:
            error_message = f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message}) 